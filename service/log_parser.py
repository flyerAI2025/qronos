"""
æ•°æ®ä¸­å¿ƒæ—¥å¿—è§£ææ¨¡å—

è¯¥æ¨¡å—ç”¨äºè§£ææ•°æ®ä¸­å¿ƒè¿è¡Œæ—¶çš„æ—¥å¿—æ–‡ä»¶ï¼Œæå–æ—¶é—´ç‚¹å’Œæ“ä½œä¿¡æ¯ã€‚
æ”¯æŒè§£æå„ç§æ•°æ®ä¸­å¿ƒæ“ä½œï¼ŒåŒ…æ‹¬æ•°æ®æ›´æ–°ã€Kçº¿è·å–ã€é¢„å¤„ç†ç­‰ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è§£ææ—¥å¿—æ–‡ä»¶æ ¼å¼
2. æå–æ“ä½œæ—¶é—´ç‚¹å’Œæ“ä½œç±»å‹
3. è¯†åˆ«æ“ä½œçŠ¶æ€ï¼ˆè¿›è¡Œä¸­ã€å®Œæˆã€å¤±è´¥ï¼‰
4. è®¡ç®—æ“ä½œè€—æ—¶
5. ç”Ÿæˆç»“æ„åŒ–çš„æ“ä½œä¿¡æ¯
6. æŒ‰ä»»åŠ¡å—åˆ†ç»„æ“ä½œï¼ˆä»¥Updateå¼€å§‹åˆ°ä¸‹ä¸€ä¸ªUpdateä¸ºä¸€ä¸ªä»»åŠ¡å—ï¼‰

æ—¥å¿—æ ¼å¼ï¼š
- æ—¶é—´æˆ³æ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS.sss +08:00:
- çŠ¶æ€æ ‡è¯†ï¼šğŸŒ€ï¼ˆè¿›è¡Œä¸­ï¼‰ã€âœ…ï¼ˆå®Œæˆï¼‰ã€âš ï¸ï¼ˆè­¦å‘Šï¼‰
- æ“ä½œç±»å‹ï¼šæ›´æ–°æ•°æ®ã€è·å–Kçº¿ã€é¢„å¤„ç†ã€åˆå¹¶ç­‰
"""

import re
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import List, Dict, Optional, Any

from utils.log_kit import get_logger

logger = get_logger()


class OperationStatus(str, Enum):
    """æ“ä½œçŠ¶æ€æšä¸¾"""
    IN_PROGRESS = "in_progress"  # è¿›è¡Œä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"  # å¤±è´¥
    SKIPPED = "skipped"  # è·³è¿‡
    UNKNOWN = "unknown"  # æœªçŸ¥


class OperationType(str, Enum):
    """æ“ä½œç±»å‹æšä¸¾"""
    UPDATE_CYCLE = "update_cycle"  # æ›´æ–°å‘¨æœŸ
    EXCHANGE_INFO = "exchange_info"  # è·å–äº¤æ˜“æ‰€ä¿¡æ¯
    MARKET_CAP_UPDATE = "market_cap_update"  # å¸‚å€¼æ•°æ®æ›´æ–°
    KLINE_UPDATE = "kline_update"  # Kçº¿æ•°æ®æ›´æ–°
    KLINE_API = "kline_api"  # Kçº¿APIè°ƒç”¨
    DATA_API_UPDATE = "data_api_update"  # Data API Kçº¿æ›´æ–°
    PREPROCESSING = "preprocessing"  # æ•°æ®é¢„å¤„ç†
    PIVOT_PROCESSING = "pivot_processing"  # Pivotè¡¨å¤„ç†
    KLINE_MERGE = "kline_merge"  # Kçº¿åˆå¹¶
    SKIP_OPERATION = "skip_operation"  # è·³è¿‡æ“ä½œ
    OTHER = "other"  # å…¶ä»–


@dataclass
class LogOperation:
    """æ—¥å¿—æ“ä½œæ•°æ®ç»“æ„"""
    timestamp: str  # æ—¶é—´æˆ³
    datetime_obj: datetime  # è§£æåçš„æ—¶é—´å¯¹è±¡
    operation_type: OperationType  # æ“ä½œç±»å‹
    status: OperationStatus  # æ“ä½œçŠ¶æ€
    description: str  # æ“ä½œæè¿°
    details: Dict[str, Any]  # è¯¦ç»†ä¿¡æ¯
    duration: Optional[float] = None  # è€—æ—¶ï¼ˆç§’ï¼‰

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "timestamp": self.timestamp,
            "operation_type": self.operation_type.value,
            "status": self.status.value,
            "description": self.description,
            "details": self.details,
            "duration": self.duration
        }


@dataclass
class TaskBlock:
    """ä»»åŠ¡å—æ•°æ®ç»“æ„"""
    id: str  # ä»»åŠ¡å—IDï¼ŒåŸºäºè¿è¡Œæ—¶é—´ç”Ÿæˆ
    start_time: str  # ä»»åŠ¡å—å¼€å§‹æ—¶é—´
    end_time: str  # ä»»åŠ¡å—ç»“æŸæ—¶é—´
    runtime: str  # åŸå§‹è¿è¡Œæ—¶é—´å­—ç¬¦ä¸²
    operations: List[LogOperation]  # è¯¥ä»»åŠ¡å—åŒ…å«çš„æ‰€æœ‰æ“ä½œ

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "id": self.id,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "runtime": self.runtime,
            "operations": [op.to_dict() for op in self.operations],
            "operation_count": len(self.operations),
            "block_duration": self._calculate_block_duration()
        }

    def _calculate_block_duration(self) -> Optional[float]:
        """è®¡ç®—ä»»åŠ¡å—æ€»è€—æ—¶"""
        if not self.operations:
            return None
        
        # è®¡ç®—å¼€å§‹æ—¶é—´åˆ°æœ€åä¸€ä¸ªæ“ä½œçš„æ—¶é—´å·®
        try:
            start_time = datetime.strptime(self.start_time, '%Y-%m-%d %H:%M:%S')
            last_op_time = max(op.datetime_obj.replace(tzinfo=None) for op in self.operations)
            duration = (last_op_time - start_time).total_seconds()
            return round(duration, 2)
        except Exception:
            return None


class DataCenterLogParser:
    """æ•°æ®ä¸­å¿ƒæ—¥å¿—è§£æå™¨"""

    # æ—¶é—´æˆ³æ­£åˆ™è¡¨è¾¾å¼
    TIMESTAMP_PATTERN = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3} \+08:00):'

    # æ“ä½œæ¨¡å¼åŒ¹é…è§„åˆ™
    OPERATION_PATTERNS = [
        # æ›´æ–°å‘¨æœŸå¼€å§‹
        {
            'pattern': r'================== Update 5m Runtime=(.+?) ===================',
            'type': OperationType.UPDATE_CYCLE,
            'status': OperationStatus.IN_PROGRESS,
            'extract_runtime': True
        },

        # è·å–äº¤æ˜“æ‰€ä¿¡æ¯
        {
            'pattern': r'Exchange Info ä¸å®æ—¶èµ„é‡‘è´¹è·å–æˆåŠŸ',
            'type': OperationType.EXCHANGE_INFO,
            'status': OperationStatus.COMPLETED
        },

        # å¸‚å€¼æ•°æ®æ›´æ–°
        {
            'pattern': r'ğŸŒ€ æ›´æ–°å¸‚å€¼æ•°æ®, å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.MARKET_CAP_UPDATE,
            'status': OperationStatus.IN_PROGRESS,
            'extract_current_time': True
        },
        {
            'pattern': r'ğŸŒ€ å¸‚å€¼æ•°æ®æ›´æ–°æˆåŠŸ, å½“å‰æ—¶é—´=(.+?), è€—æ—¶=(.+?)åˆ†é’Ÿ',
            'type': OperationType.MARKET_CAP_UPDATE,
            'status': OperationStatus.COMPLETED,
            'extract_duration': True
        },

        # Kçº¿æ•°æ®æ›´æ–°å¼€å§‹
        {
            'pattern': r'ğŸŒ€ å¼€å§‹æ›´æ–°å¸å®‰ (spot|swap) K çº¿, äº¤æ˜“å¯¹æ•°é‡=(\d+), å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.KLINE_UPDATE,
            'status': OperationStatus.IN_PROGRESS,
            'extract_details': True
        },

        # Kçº¿APIå®Œæˆ
        {
            'pattern': r'âœ… Binance (spot|swap) API, è·å– 5m æˆåŠŸ, Resample å¹¶æ›´æ–° 1h æˆåŠŸ, è€—æ—¶=(.+?)ç§’, å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.KLINE_API,
            'status': OperationStatus.COMPLETED,
            'extract_duration': True,
            'extract_details': True
        },

        # Data API Kçº¿æ›´æ–°å¼€å§‹
        {
            'pattern': r'ğŸŒ€ å¼€å§‹è¯·æ±‚ Data API K çº¿, å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.DATA_API_UPDATE,
            'status': OperationStatus.IN_PROGRESS,
            'extract_data_api_time': True
        },

        # Data APIè¯·æ±‚å¤±è´¥é‡è¯•
        {
            'pattern': r'âŒ è¯·æ±‚ DataAPI URL å¤±è´¥, é‡è¯•ä¸­, å½“å‰æ—¶é—´=(.+?),',
            'type': OperationType.DATA_API_UPDATE,
            'status': OperationStatus.FAILED,
            'extract_data_api_time': True
        },

        # Data API URLå°±ç»ª
        {
            'pattern': r'âœ… DataAPI URL å°±ç»ª, DataAPI æ—¶é—´æˆ³=(.+?), å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.DATA_API_UPDATE,
            'status': OperationStatus.COMPLETED,
            'extract_data_api_ready': True
        },

        # Data APIç°è´§URLè¯·æ±‚
        {
            'pattern': r'ğŸŒ€ data_api_spot=(.+)',
            'type': OperationType.DATA_API_UPDATE,
            'status': OperationStatus.IN_PROGRESS,
            'extract_data_api_url': True
        },

        # Data APIåˆçº¦URLè¯·æ±‚
        {
            'pattern': r'ğŸŒ€ data_api_swap=(.+)',
            'type': OperationType.DATA_API_UPDATE,
            'status': OperationStatus.IN_PROGRESS,
            'extract_data_api_url': True
        },

        # Data APIç°è´§æ•°æ®æ›´æ–°æˆåŠŸ
        {
            'pattern': r'âœ… è·å–å¹¶åˆå¹¶ DataAPI æ•°æ® spot æˆåŠŸ, å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.DATA_API_UPDATE,
            'status': OperationStatus.COMPLETED,
            'extract_data_api_success': True
        },

        # Data APIåˆçº¦æ•°æ®æ›´æ–°æˆåŠŸ
        {
            'pattern': r'âœ… è·å–å¹¶åˆå¹¶ DataAPI æ•°æ® swap æˆåŠŸ, å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.DATA_API_UPDATE,
            'status': OperationStatus.COMPLETED,
            'extract_data_api_success': True
        },

        # é¢„å¤„ç†å¼€å§‹
        {
            'pattern': r'ğŸŒ€ å¼€å§‹é¢„å¤„ç† (spot|swap), å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.PREPROCESSING,
            'status': OperationStatus.IN_PROGRESS,
            'extract_details': True
        },

        # é¢„å¤„ç†æ‰¹æ¬¡å®Œæˆ
        {
            'pattern': r'é¢„å¤„ç† Market Dict (spot|swap) batch(\d+) å®Œæˆ, äº¤æ˜“å¯¹=(.+?) -- (.+?), æ•°æ®æº=(.+?), å½“å‰æ—¶é—´=(.+?), è€—æ—¶ (.+?) ç§’',
            'type': OperationType.PREPROCESSING,
            'status': OperationStatus.IN_PROGRESS,
            'extract_batch_details': True
        },

        # é¢„å¤„ç†å®Œæˆ
        {
            'pattern': r'âœ… é¢„å¤„ç† Market Dict (spot|swap) å®Œæˆ, å½“å‰æ—¶é—´=(.+?), è€—æ—¶ (.+?) ç§’',
            'type': OperationType.PREPROCESSING,
            'status': OperationStatus.COMPLETED,
            'extract_duration': True,
            'extract_details': True
        },

        # Pivotè¡¨å¤„ç†
        {
            'pattern': r'ğŸŒ€ ç”Ÿæˆ Market Pivot (spot|swap) (\d+) å®Œæˆ',
            'type': OperationType.PIVOT_PROCESSING,
            'status': OperationStatus.IN_PROGRESS,
            'extract_details': True
        },
        {
            'pattern': r'âœ… é¢„å¤„ç† Pivot Table (spot|swap) å®Œæˆ, å½“å‰æ—¶é—´=(.+?), è€—æ—¶ (.+?) ç§’',
            'type': OperationType.PIVOT_PROCESSING,
            'status': OperationStatus.COMPLETED,
            'extract_duration': True,
            'extract_details': True
        },

        # Kçº¿åˆå¹¶
        {
            'pattern': r'ğŸŒ€ å¼€å§‹åˆå¹¶å¸å®‰ (spot|swap) 5m K çº¿, å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.KLINE_MERGE,
            'status': OperationStatus.IN_PROGRESS,
            'extract_details': True
        },
        {
            'pattern': r'âœ… åˆå¹¶å¸å®‰ (spot|swap) 5m K çº¿æˆåŠŸ, å½“å‰æ—¶é—´=(.+)',
            'type': OperationType.KLINE_MERGE,
            'status': OperationStatus.COMPLETED,
            'extract_details': True
        },

        # è·³è¿‡æ“ä½œ
        {
            'pattern': r'ğŸŒ€ Runtime=(.+?),ä¸åœ¨ Offset=(.+?) ä¸­ï¼Œä¼‘æ¯ 60s åï¼Œè·³è¿‡',
            'type': OperationType.SKIP_OPERATION,
            'status': OperationStatus.SKIPPED,
            'extract_skip_details': True
        }
    ]

    def __init__(self):
        """åˆå§‹åŒ–è§£æå™¨"""
        pass

    def parse_log_file(self, log_file_path: Path, hours: Optional[int] = None) -> List[LogOperation]:
        """
        è§£ææ—¥å¿—æ–‡ä»¶
        
        Args:
            log_file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            hours: è·å–æœ€è¿‘å¤šå°‘å°æ—¶çš„æ—¥å¿—ï¼ŒNoneè¡¨ç¤ºè§£æå…¨éƒ¨
            
        Returns:
            è§£æåçš„æ“ä½œåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹è§£ææ—¥å¿—æ–‡ä»¶: {log_file_path}")

        if not log_file_path.exists():
            logger.error(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file_path}")
            return []

        operations = []

        try:
            with open(log_file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            logger.info(f"è¯»å–æ—¥å¿—è¡Œæ•°: {len(lines)}")

            # è®¡ç®—æ—¶é—´é˜ˆå€¼ï¼ˆå¦‚æœæŒ‡å®šäº†å°æ—¶æ•°ï¼‰
            time_threshold = None
            if hours is not None:
                from datetime import timedelta, timezone
                # ä½¿ç”¨å½“å‰æ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ UTC+8ï¼‰
                current_time = datetime.now(timezone(timedelta(hours=8)))
                time_threshold = current_time - timedelta(hours=hours)
                logger.info(f"æ—¶é—´è¿‡æ»¤é˜ˆå€¼: {time_threshold.strftime('%Y-%m-%d %H:%M:%S %z')} (æœ€è¿‘{hours}å°æ—¶)")

            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                if not line:
                    continue

                operation = self._parse_log_line(line)
                if operation:
                    # å¦‚æœæŒ‡å®šäº†æ—¶é—´èŒƒå›´ï¼Œè¿›è¡Œæ—¶é—´è¿‡æ»¤
                    if time_threshold is not None:
                        # æ¯”è¾ƒæ“ä½œæ—¶é—´ä¸é˜ˆå€¼
                        if operation.datetime_obj < time_threshold:
                            continue  # è·³è¿‡è¶…å‡ºæ—¶é—´èŒƒå›´çš„æ“ä½œ
                    
                    operations.append(operation)

            logger.info(f"è§£æå®Œæˆï¼Œå…±æå– {len(operations)} ä¸ªæ“ä½œ")
            if hours is not None:
                logger.info(f"æ—¶é—´è¿‡æ»¤èŒƒå›´: æœ€è¿‘ {hours} å°æ—¶")

            # æŒ‰æ—¶é—´æ’åº
            operations.sort(key=lambda x: x.datetime_obj)

            return operations

        except Exception as e:
            logger.error(f"è§£ææ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def _parse_log_line(self, line: str) -> Optional[LogOperation]:
        """
        è§£æå•è¡Œæ—¥å¿—
        
        Args:
            line: æ—¥å¿—è¡Œå†…å®¹
            
        Returns:
            è§£æåçš„æ“ä½œå¯¹è±¡ï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å›None
        """
        # æå–æ—¶é—´æˆ³
        timestamp_match = re.match(self.TIMESTAMP_PATTERN, line)
        if not timestamp_match:
            return None

        timestamp_str = timestamp_match.group(1)
        content = line[len(timestamp_match.group(0)):].strip()

        # è§£ææ—¶é—´æˆ³
        try:
            datetime_obj = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S.%f %z')
        except ValueError:
            logger.warning(f"æ— æ³•è§£ææ—¶é—´æˆ³: {timestamp_str}")
            return None

        # åŒ¹é…æ“ä½œæ¨¡å¼
        for pattern_info in self.OPERATION_PATTERNS:
            match = re.search(pattern_info['pattern'], content)
            if match:
                return self._create_operation(
                    timestamp_str, datetime_obj, content, pattern_info, match
                )

        # æœªåŒ¹é…åˆ°å·²çŸ¥æ¨¡å¼ï¼Œå½’ç±»ä¸ºå…¶ä»–
        return LogOperation(
            timestamp=timestamp_str,
            datetime_obj=datetime_obj,
            operation_type=OperationType.OTHER,
            status=OperationStatus.UNKNOWN,
            description=content,
            details={}
        )

    @staticmethod
    def _create_operation(timestamp_str: str, datetime_obj: datetime,
                          content: str, pattern_info: Dict, match: re.Match) -> LogOperation:
        """
        æ ¹æ®åŒ¹é…ç»“æœåˆ›å»ºæ“ä½œå¯¹è±¡
        
        Args:
            timestamp_str: æ—¶é—´æˆ³å­—ç¬¦ä¸²
            datetime_obj: æ—¶é—´å¯¹è±¡
            content: æ—¥å¿—å†…å®¹
            pattern_info: æ¨¡å¼ä¿¡æ¯
            match: æ­£åˆ™åŒ¹é…ç»“æœ
            
        Returns:
            æ“ä½œå¯¹è±¡
        """
        details = {}
        duration = None

        # æå–è¿è¡Œæ—¶æ—¶é—´
        if pattern_info.get('extract_runtime'):
            details['runtime'] = match.group(1)

        # æå–å½“å‰æ—¶é—´
        if pattern_info.get('extract_current_time'):
            details['current_time'] = match.group(1)

        # æå–Data APIæ—¶é—´
        if pattern_info.get('extract_data_api_time'):
            details['current_time'] = match.group(1)

        # æå–Data APIå°±ç»ªä¿¡æ¯
        if pattern_info.get('extract_data_api_ready'):
            details['data_api_timestamp'] = match.group(1)
            details['current_time'] = match.group(2)

        # æå–Data API URLä¿¡æ¯
        if pattern_info.get('extract_data_api_url'):
            details['data_api_url'] = match.group(1)
            # ä»æè¿°ä¸­æå–APIç±»å‹ï¼ˆspotæˆ–swapï¼‰
            if 'data_api_spot' in content:
                details['api_type'] = 'spot'
            elif 'data_api_swap' in content:
                details['api_type'] = 'swap'

        # æå–Data APIæˆåŠŸä¿¡æ¯
        if pattern_info.get('extract_data_api_success'):
            details['current_time'] = match.group(1)
            # ä»æè¿°ä¸­æå–æ•°æ®ç±»å‹ï¼ˆspotæˆ–swapï¼‰
            if ' spot ' in content:
                details['data_type'] = 'spot'
            elif ' swap ' in content:
                details['data_type'] = 'swap'

        # æå–è€—æ—¶ä¿¡æ¯
        if pattern_info.get('extract_duration'):
            try:
                if 'spot' in content or 'swap' in content:
                    # Kçº¿APIçš„è€—æ—¶æ ¼å¼ï¼šè€—æ—¶=16.8ç§’
                    duration_str = match.group(2)
                    if 'ç§’' in duration_str:
                        duration = float(duration_str.replace('ç§’', ''))
                    elif 'åˆ†é’Ÿ' in duration_str:
                        duration = float(duration_str.replace('åˆ†é’Ÿ', '')) * 60
                else:
                    # å¸‚å€¼æ•°æ®çš„è€—æ—¶æ ¼å¼ï¼šè€—æ—¶=0.07åˆ†é’Ÿ
                    duration_str = match.group(2)
                    if 'åˆ†é’Ÿ' in duration_str:
                        duration = float(duration_str.replace('åˆ†é’Ÿ', '')) * 60
                    elif 'ç§’' in duration_str:
                        duration = float(duration_str.replace('ç§’', ''))
            except (IndexError, ValueError) as e:
                logger.warning(f"è§£æè€—æ—¶å¤±è´¥: {e}")

        # æå–è¯¦ç»†ä¿¡æ¯
        if pattern_info.get('extract_details'):
            if pattern_info['type'] == OperationType.KLINE_UPDATE:
                details['market_type'] = match.group(1)  # spot æˆ– swap
                details['pair_count'] = int(match.group(2))
                details['current_time'] = match.group(3)
            elif pattern_info['type'] == OperationType.KLINE_API:
                details['market_type'] = match.group(1)
                details['current_time'] = match.group(3)
            elif pattern_info['type'] == OperationType.PREPROCESSING:
                if 'Market Dict' not in content:  # å¼€å§‹é¢„å¤„ç†
                    details['market_type'] = match.group(1)
                    details['current_time'] = match.group(2)
                else:  # é¢„å¤„ç†å®Œæˆ
                    details['market_type'] = match.group(1)
                    details['current_time'] = match.group(2)
            elif pattern_info['type'] == OperationType.PIVOT_PROCESSING:
                if 'ç”Ÿæˆ' in content:
                    details['market_type'] = match.group(1)
                    details['year'] = match.group(2)
                else:
                    details['market_type'] = match.group(1)
                    details['current_time'] = match.group(2)
            elif pattern_info['type'] == OperationType.KLINE_MERGE:
                details['market_type'] = match.group(1)
                details['current_time'] = match.group(2)

        # æå–æ‰¹æ¬¡å¤„ç†è¯¦ç»†ä¿¡æ¯
        if pattern_info.get('extract_batch_details'):
            details.update({
                'market_type': match.group(1),
                'batch_number': int(match.group(2)),
                'start_pair': match.group(3),
                'end_pair': match.group(4),
                'data_source': match.group(5),
                'current_time': match.group(6),
                'batch_duration': float(match.group(7))
            })

        # æå–è·³è¿‡æ“ä½œè¯¦ç»†ä¿¡æ¯
        if pattern_info.get('extract_skip_details'):
            details.update({
                'runtime': match.group(1),
                'offset_range': match.group(2)
            })

        return LogOperation(
            timestamp=timestamp_str,
            datetime_obj=datetime_obj,
            operation_type=pattern_info['type'],
            status=pattern_info['status'],
            description=content,
            details=details,
            duration=duration
        )

    @staticmethod
    def group_operations_by_task_blocks(operations: List[LogOperation]) -> List[TaskBlock]:
        """
        æŒ‰ä»»åŠ¡å—åˆ†ç»„æ“ä½œ
        
        ä»¥æ¯ä¸ª "Update 5m Runtime" æˆ– "è·³è¿‡æ“ä½œ" ä¸ºèµ·ç‚¹ï¼Œå°†å…¶åˆ°ä¸‹ä¸€ä¸ªå‘¨æœŸå¼€å§‹ä¹‹é—´çš„æ‰€æœ‰æ“ä½œå½’ä¸ºä¸€ä¸ªä»»åŠ¡å—ã€‚
        
        Args:
            operations: æ“ä½œåˆ—è¡¨
            
        Returns:
            ä»»åŠ¡å—åˆ—è¡¨
        """
        if not operations:
            return []

        task_blocks = []
        current_block_operations = []
        current_runtime = None
        current_start_time = None

        for operation in operations:
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ–°ä»»åŠ¡å—çš„å¼€å§‹ï¼šUpdateå‘¨æœŸå¼€å§‹ æˆ– è·³è¿‡æ“ä½œ
            is_new_block_start = (
                operation.operation_type == OperationType.UPDATE_CYCLE or 
                operation.operation_type == OperationType.SKIP_OPERATION
            )
            
            if is_new_block_start:
                # é‡åˆ°æ–°çš„ä»»åŠ¡å—å¼€å§‹ï¼Œå…ˆä¿å­˜å½“å‰ä»»åŠ¡å—ï¼ˆå¦‚æœæœ‰ï¼‰
                if current_block_operations and current_runtime and current_start_time:
                    task_block = DataCenterLogParser._create_task_block(
                        current_runtime, current_start_time, current_block_operations
                    )
                    task_blocks.append(task_block)
                
                # å¼€å§‹æ–°çš„ä»»åŠ¡å—
                if operation.operation_type == OperationType.UPDATE_CYCLE:
                    # æ­£å¸¸çš„Updateå‘¨æœŸ
                    current_runtime = operation.details.get('runtime', operation.timestamp)
                else:
                    # è·³è¿‡æ“ä½œï¼šä»detailsä¸­æå–runtime
                    current_runtime = operation.details.get('runtime', operation.timestamp)
                
                current_start_time = operation.timestamp
                current_block_operations = [operation]
            else:
                # æ·»åŠ åˆ°å½“å‰ä»»åŠ¡å—
                if current_block_operations is not None:
                    current_block_operations.append(operation)

        # å¤„ç†æœ€åä¸€ä¸ªä»»åŠ¡å—
        if current_block_operations and current_runtime and current_start_time:
            task_block = DataCenterLogParser._create_task_block(
                current_runtime, current_start_time, current_block_operations
            )
            task_blocks.append(task_block)

        return task_blocks

    @staticmethod
    def _create_task_block(runtime: str, start_time: str, operations: List[LogOperation]) -> TaskBlock:
        """
        åˆ›å»ºä»»åŠ¡å—å¯¹è±¡
        
        Args:
            runtime: è¿è¡Œæ—¶é—´å­—ç¬¦ä¸²
            start_time: å¼€å§‹æ—¶é—´
            operations: æ“ä½œåˆ—è¡¨
            
        Returns:
            ä»»åŠ¡å—å¯¹è±¡
        """
        # ç”Ÿæˆä»»åŠ¡å—IDï¼ˆåŸºäºè¿è¡Œæ—¶é—´ï¼‰
        try:
            # å°è¯•è§£æè¿è¡Œæ—¶é—´å¹¶æ ¼å¼åŒ–ä¸ºID
            runtime_obj = datetime.fromisoformat(runtime.replace('+08:00', '+08:00'))
            task_id = runtime_obj.strftime('%Y%m%d%H%M%S')
        except Exception:
            # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²ç”ŸæˆID
            import hashlib
            task_id = hashlib.md5(runtime.encode()).hexdigest()[:12]

        # è®¡ç®—ç»“æŸæ—¶é—´ï¼ˆæœ€åä¸€ä¸ªæ“ä½œçš„æ—¶é—´ï¼‰
        if operations:
            end_time = max(op.timestamp for op in operations)
        else:
            end_time = start_time

        # æ ¼å¼åŒ–å¼€å§‹æ—¶é—´ï¼ˆç§»é™¤æ¯«ç§’å’Œæ—¶åŒºï¼‰
        try:
            start_datetime = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S.%f %z')
            formatted_start_time = start_datetime.strftime('%Y-%m-%d %H:%M:%S')
        except Exception:
            formatted_start_time = start_time

        # æ ¼å¼åŒ–ç»“æŸæ—¶é—´
        try:
            end_datetime = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S.%f %z')
            formatted_end_time = end_datetime.strftime('%Y-%m-%d %H:%M:%S')
        except Exception:
            formatted_end_time = end_time

        return TaskBlock(
            id=task_id,
            start_time=formatted_start_time,
            end_time=formatted_end_time,
            runtime=runtime,
            operations=operations
        )

def merge_duplicate_task_blocks(task_blocks: List[TaskBlock], merge_window_minutes: int = 2) -> List[TaskBlock]:
    """
    åˆå¹¶è·³è¿‡æ“ä½œä¸­ç›¸åŒRuntimeçš„ä»»åŠ¡å—
    
    é’ˆå¯¹æ‰€æœ‰åŒ…å«è·³è¿‡æ“ä½œçš„ä»»åŠ¡å—è¿›è¡Œåˆå¹¶ï¼ŒåŸºäºç›¸åŒçš„Runtimeå€¼å’Œæ—¶é—´çª—å£è¿›è¡Œåˆå¹¶ã€‚
    
    Args:
        task_blocks: ä»»åŠ¡å—åˆ—è¡¨
        merge_window_minutes: åˆå¹¶æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿï¼‰
        
    Returns:
        åˆå¹¶åçš„ä»»åŠ¡å—åˆ—è¡¨
    """
    if not task_blocks:
        return []
    
    # åˆ†ç¦»è·³è¿‡æ“ä½œå’Œå…¶ä»–æ“ä½œ
    skip_blocks = []
    other_blocks = []
    
    for block in task_blocks:
        # æ£€æŸ¥ä»»åŠ¡å—æ˜¯å¦åŒ…å«è·³è¿‡æ“ä½œï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªè·³è¿‡æ“ä½œï¼‰
        has_skip_operations = any(op.operation_type == OperationType.SKIP_OPERATION for op in block.operations)
        if has_skip_operations:
            # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ“ä½œéƒ½æ˜¯è·³è¿‡æ“ä½œ
            all_skip = all(op.operation_type == OperationType.SKIP_OPERATION for op in block.operations)
            if all_skip:
                skip_blocks.append(block)
            else:
                # æ··åˆç±»å‹çš„ä»»åŠ¡å—ï¼Œæš‚æ—¶ä¸åˆå¹¶
                other_blocks.append(block)
        else:
            other_blocks.append(block)
    
    logger.info(f"æ‰¾åˆ° {len(skip_blocks)} ä¸ªè·³è¿‡æ“ä½œä»»åŠ¡å—ï¼Œ{len(other_blocks)} ä¸ªå…¶ä»–ä»»åŠ¡å—")
    
    # å¯¹è·³è¿‡æ“ä½œæŒ‰Runtimeåˆ†ç»„
    runtime_groups = {}
    for block in skip_blocks:
        # è·å–ä»»åŠ¡å—ä¸­ç¬¬ä¸€ä¸ªè·³è¿‡æ“ä½œçš„runtimeï¼ˆåŒä¸€ä»»åŠ¡å—ä¸­æ‰€æœ‰è·³è¿‡æ“ä½œçš„runtimeåº”è¯¥ç›¸åŒï¼‰
        runtime = None
        for operation in block.operations:
            if operation.operation_type == OperationType.SKIP_OPERATION:
                runtime = operation.details.get('runtime', '')
                break
        
        if runtime:
            if runtime not in runtime_groups:
                runtime_groups[runtime] = []
            runtime_groups[runtime].append(block)
    
    merged_skip_blocks = []
    merge_count = 0
    
    # åˆå¹¶ç›¸åŒRuntimeçš„è·³è¿‡æ“ä½œ
    for runtime, blocks in runtime_groups.items():
        if len(blocks) == 1:
            # åªæœ‰ä¸€ä¸ªå—ï¼Œæ— éœ€åˆå¹¶
            merged_skip_blocks.append(blocks[0])
        else:
            # æœ‰å¤šä¸ªç›¸åŒRuntimeçš„å—ï¼Œéœ€è¦åˆå¹¶
            logger.info(f"å‘ç° {len(blocks)} ä¸ªç›¸åŒRuntimeçš„è·³è¿‡æ“ä½œä»»åŠ¡å—: {runtime}")
            
            # æŒ‰å¼€å§‹æ—¶é—´æ’åº
            blocks.sort(key=lambda b: datetime.strptime(b.start_time, '%Y-%m-%d %H:%M:%S'))
            
            # å°†æ‰€æœ‰ç›¸åŒRuntimeçš„ä»»åŠ¡å—åˆå¹¶ä¸ºä¸€ä¸ª
            merged_block = blocks[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå—ä½œä¸ºåŸºç¡€
            
            # å°†åç»­æ‰€æœ‰å—çš„æ“ä½œæ·»åŠ åˆ°ç¬¬ä¸€ä¸ªå—ä¸­
            for next_block in blocks[1:]:
                # è®¡ç®—æ—¶é—´å·®ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
                current_time = datetime.strptime(merged_block.start_time, '%Y-%m-%d %H:%M:%S')
                next_time = datetime.strptime(next_block.start_time, '%Y-%m-%d %H:%M:%S')
                time_diff_minutes = (next_time - current_time).total_seconds() / 60
                
                logger.info(f"åˆå¹¶è·³è¿‡æ“ä½œä»»åŠ¡å— Runtime={runtime}: "
                           f"å°† {next_block.start_time} çš„ {len(next_block.operations)} ä¸ªæ“ä½œåˆå¹¶åˆ°ä¸»å—")
                
                # åˆå¹¶æ“ä½œåˆ—è¡¨
                merged_block.operations.extend(next_block.operations)
                merge_count += 1
            
            # æ›´æ–°ç»“æŸæ—¶é—´ä¸ºæ‰€æœ‰æ“ä½œä¸­çš„æœ€æ–°æ—¶é—´
            if merged_block.operations:
                latest_time = max(op.timestamp for op in merged_block.operations)
                try:
                    latest_datetime = datetime.strptime(latest_time, '%Y-%m-%d %H:%M:%S.%f %z')
                    merged_block.end_time = latest_datetime.strftime('%Y-%m-%d %H:%M:%S')
                except Exception:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªå—çš„ç»“æŸæ—¶é—´
                    merged_block.end_time = blocks[-1].end_time
            
            # æŒ‰æ—¶é—´æˆ³é‡æ–°æ’åºæ‰€æœ‰æ“ä½œ
            merged_block.operations.sort(key=lambda op: op.datetime_obj)
            
            # æ·»åŠ åˆå¹¶åçš„å—
            merged_skip_blocks.append(merged_block)
            
            logger.info(f"è·³è¿‡æ“ä½œä»»åŠ¡å—åˆå¹¶å®Œæˆï¼ŒRuntime={runtime}: "
                       f"åˆå¹¶äº† {len(blocks)} ä¸ªä»»åŠ¡å— -> 1ä¸ªä»»åŠ¡å—ï¼ŒåŒ…å« {len(merged_block.operations)} ä¸ªæ“ä½œ")
    
    # åˆå¹¶æ‰€æœ‰ç»“æœï¼šå…¶ä»–ä»»åŠ¡å— + åˆå¹¶åçš„è·³è¿‡æ“ä½œä»»åŠ¡å—
    all_merged_blocks = other_blocks + merged_skip_blocks
    
    # æŒ‰å¼€å§‹æ—¶é—´é‡æ–°æ’åºæ‰€æœ‰ä»»åŠ¡å—
    all_merged_blocks.sort(key=lambda b: datetime.strptime(b.start_time, '%Y-%m-%d %H:%M:%S'))
    
    original_skip_count = len(skip_blocks)
    final_skip_count = len(merged_skip_blocks)
    
    logger.info(f"è·³è¿‡æ“ä½œåˆå¹¶å®Œæˆ: åŸ {original_skip_count} ä¸ªè·³è¿‡æ“ä½œä»»åŠ¡å— -> åˆå¹¶å {final_skip_count} ä¸ª (åˆå¹¶äº† {merge_count} ä¸ª)")
    logger.info(f"æ€»ä»»åŠ¡å—: åŸ {len(task_blocks)} ä¸ª -> æœ€ç»ˆ {len(all_merged_blocks)} ä¸ª")
    
    return all_merged_blocks


def parse_data_center_logs(framework_id: str, hours: Optional[int] = 24) -> Dict[str, Any]:
    """
    è§£ææŒ‡å®šæ•°æ®ä¸­å¿ƒæ¡†æ¶çš„æ—¥å¿—
    
    Args:
        framework_id: æ•°æ®ä¸­å¿ƒæ¡†æ¶ID
        hours: è·å–æœ€è¿‘å¤šå°‘å°æ—¶çš„æ—¥å¿—ï¼ŒNoneè¡¨ç¤ºè·å–å…¨éƒ¨
        
    Returns:
        è§£æç»“æœå­—å…¸ï¼ŒåŒ…å«ä»»åŠ¡å—åˆ†ç»„çš„æ•°æ®
    """
    logger.info(f"è§£ææ•°æ®ä¸­å¿ƒæ—¥å¿—: framework_id={framework_id}, hours={hours}")

    from db.db_ops import get_framework_status

    # è·å–æ¡†æ¶çŠ¶æ€
    framework_status = get_framework_status(framework_id)
    if not framework_status or not framework_status.path:
        logger.error(f"æ•°æ®ä¸­å¿ƒæ¡†æ¶æœªæ‰¾åˆ°æˆ–è·¯å¾„ä¸ºç©º: {framework_id}")
        return {"error": "æ•°æ®ä¸­å¿ƒæ¡†æ¶æœªæ‰¾åˆ°"}

    framework_path = Path(framework_status.path)

    # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
    log_files = []

    # æŸ¥æ‰¾logsç›®å½•ä¸‹çš„realtime_data.out-{pm2_id}.logæ–‡ä»¶
    logs_dir = framework_path / "logs"
    if logs_dir.exists():
        # ä½¿ç”¨globæ¨¡å¼åŒ¹é…realtime_data.out-*.logæ–‡ä»¶
        log_pattern = "realtime_data.out-*.log"
        matched_files = list(logs_dir.glob(log_pattern))
        
        # è¿‡æ»¤æ‰æ—¥å¿—è½®è½¬æ–‡ä»¶
        for log_file in matched_files:
            # è·³è¿‡PM2 logrotateæ’ä»¶ç”Ÿæˆçš„æ‹†åˆ†æ–‡ä»¶ï¼š
            # 1. æ•°å­—åç¼€ï¼šrealtime_data.out-9.log.1, realtime_data.out-9.log.2
            # 2. æ—¶é—´æˆ³åç¼€ï¼šrealtime_data.out-9__2025-07-12_00-15-46.log
            file_name = log_file.name
            
            # è·³è¿‡æ•°å­—åç¼€çš„è½®è½¬æ–‡ä»¶
            if any(file_name.endswith(f'.{i}') for i in range(1, 10)):
                logger.debug(f"è·³è¿‡æ•°å­—åç¼€è½®è½¬æ–‡ä»¶: {file_name}")
                continue
                
            # è·³è¿‡åŒ…å«åŒä¸‹åˆ’çº¿æ—¶é—´æˆ³çš„æ‹†åˆ†æ–‡ä»¶
            if '__' in file_name and file_name.count('__') >= 1:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸæ—¶é—´æ ¼å¼
                if re.search(r'__\d{4}-\d{2}-\d{2}_\d{2}-\d{2}-\d{2}', file_name):
                    logger.debug(f"è·³è¿‡æ—¶é—´æˆ³æ‹†åˆ†æ–‡ä»¶: {file_name}")
                    continue
            
            # åªå¤„ç†ä¸»æ—¥å¿—æ–‡ä»¶
            log_files.append(log_file)
            logger.info(f"æ‰¾åˆ°æ•°æ®ä¸­å¿ƒæ—¥å¿—æ–‡ä»¶: {log_file}")

    if not log_files:
        logger.warning(f"æœªæ‰¾åˆ°æ•°æ®ä¸­å¿ƒæ—¥å¿—æ–‡ä»¶: {framework_path}")
        return {"error": "æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶"}

    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ—¥å¿—æ–‡ä»¶
    log_file = log_files[0]
    logger.info(f"ä½¿ç”¨æ—¥å¿—æ–‡ä»¶: {log_file}")

    # è§£ææ—¥å¿—
    parser = DataCenterLogParser()
    operations = parser.parse_log_file(log_file, hours=hours)

    if not operations:
        return {"error": "æ—¥å¿—è§£æå¤±è´¥æˆ–æ— æœ‰æ•ˆæ“ä½œ"}

    # è¿‡æ»¤æ‰descriptionä¸ºç©ºçš„æ“ä½œ
    operations = [op for op in operations if op.description and op.description.strip()]
    logger.info(f"è¿‡æ»¤ç©ºæè¿°åï¼Œå‰©ä½™ {len(operations)} ä¸ªæ“ä½œ")

    if not operations:
        return {"error": "è¿‡æ»¤åæ— æœ‰æ•ˆæ“ä½œ"}

    # æŒ‰ä»»åŠ¡å—åˆ†ç»„æ“ä½œ
    task_blocks = parser.group_operations_by_task_blocks(operations)
    
    # åˆå¹¶å…·æœ‰ç›¸åŒIDçš„ä»»åŠ¡å—
    task_blocks = merge_duplicate_task_blocks(task_blocks, merge_window_minutes=2)

    # æ„å»ºè¿”å›ç»“æœ
    result = {
        "framework_info": {
            "framework_id": framework_id,
            "framework_name": framework_status.framework_name,
            "log_file": str(log_file),
            "framework_path": str(framework_path)
        },
        "task_blocks": [block.to_dict() for block in task_blocks],
        "task_blocks_count": len(task_blocks)
    }

    logger.info(f"æ—¥å¿—è§£æå®Œæˆï¼Œå…± {len(operations)} ä¸ªæ“ä½œï¼Œ{len(task_blocks)} ä¸ªä»»åŠ¡å—")
    if hours is not None:
        logger.info(f"æ—¶é—´èŒƒå›´: æœ€è¿‘ {hours} å°æ—¶")

    return result
